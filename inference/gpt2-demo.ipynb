{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ce4ce2-dda6-47e8-b016-9966a2b66b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import contextlib\n",
    "import transformers\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f48fcab-9733-4a7e-af25-cfd12ba8ea3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24247a88-fdef-4901-8548-3ed55cbc4163",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "- Can we use GPT-2 to predict media effect between partisans? --> human language processing / writing assitive tool\n",
    "- Can we infer the partisanship from the tweet? --> bayesian inference\n",
    "- Can we quantify the level of mutual understanding between partisans? --> semantic similarity\n",
    "- Can we extract partisan-specific opinions from fine-tuned GPT-2? --> opinion generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d0714-39e7-4164-b114-c80a4f2475ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up\n",
    "    we fine-tune two GPT-2 models, call them \n",
    "         - dem_model: fine-tuned on 10M tweets from Democrats\n",
    "         - repub_model: fine-tuned on 4M tweets from Republicans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97193b72-28cf-4156-86c1-ea065e5c9f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dems_tokenizer = GPT2Tokenizer.from_pretrained('../models/gpt2_dems_full_data_single_follow/checkpoint-1367160/')\n",
    "# dem_model = GPT2LMHeadModel.from_pretrained('../models/gpt2_dems_full_data_single_follow/checkpoint-1367160/')\n",
    "\n",
    "# dems_tokenizer = GPT2Tokenizer.from_pretrained('../models/pretrained_gpt2_dems_4M_full_data_single_follow/checkpoint-489680/')\n",
    "# dem_model = GPT2LMHeadModel.from_pretrained('../models/pretrained_gpt2_dems_4M_full_data_single_follow/checkpoint-489680/')\n",
    "\n",
    "# repub_tokenizer = GPT2Tokenizer.from_pretrained('../models/gpt2_repubs_full_data_single_follow/checkpoint-490180/')\n",
    "# repub_model = GPT2LMHeadModel.from_pretrained('../models/gpt2_repubs_full_data_single_follow/checkpoint-490180/')\n",
    "\n",
    "dems_tokenizer = GPT2Tokenizer.from_pretrained('../train_lm/models/pretrained_gpt2_2019_dem/')\n",
    "dem_model = GPT2LMHeadModel.from_pretrained('../train_lm/models/pretrained_gpt2_2019_dem')\n",
    "\n",
    "repub_tokenizer = GPT2Tokenizer.from_pretrained('../train_lm/models/pretrained_gpt2_2019_repub/')\n",
    "repub_model = GPT2LMHeadModel.from_pretrained('../train_lm/models/pretrained_gpt2_2019_repub/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f8a664-f282-4819-9e8a-109462522d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efad0194-fea2-4390-95ff-1486ef42b7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def tokenize_sentences(tokenizer, model):\n",
    "#     # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "#     # model.eval()\n",
    "#     for sentence in sentences:\n",
    "#         sentence = sentence.lower()\n",
    "#         #tokenized_sentences.append(' '.join(tokenizer.tokenize(sentence)))\n",
    "#         tokenized_sentences.append(' '.join([tokenizer.bos_token] + tokenizer.tokenize(sentence)))\n",
    "\n",
    "#     for s in tokenized_sentences:\n",
    "#         print(s)\n",
    "\n",
    "\n",
    "# get surprisals\n",
    "def score_sentence_token_surprisals(sentence, model, tokenizer):\n",
    "    # sentence = sentence.lower()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenizer.bos_token) + tokenizer.encode(sentence)).unsqueeze(0)\n",
    "    prediction_scores = model(input_ids)[0]\n",
    "    \n",
    "    # print(prediction_scores.shape)\n",
    "\n",
    "    scores = []\n",
    "    for index in range(input_ids.shape[1]):\n",
    "\n",
    "        scores.append(str(-1 * math.log2(torch.nn.Softmax(0)(prediction_scores[0][index-1])[input_ids[0][index]].item())))\n",
    "\n",
    "    return(zip(tokens,scores[1:len(scores)]))\n",
    "\n",
    "def show_word_surprisals(sentence, model, tokenizer, positions=None):\n",
    "    x = list(score_sentence_token_surprisals(sentence, model, tokenizer))\n",
    "    total_surprisal = 0\n",
    "    surprisal_list = []\n",
    "    for i in range(len(x)):\n",
    "        if positions == None or i in positions:\n",
    "            # print(\"\\t\".join(x[i]))\n",
    "            surprisal_list.append(x[i])\n",
    "            total_surprisal += float(x[i][1])\n",
    "    # print(\"total surprisal\", total_surprisal)\n",
    "    # print()\n",
    "    surprisal_list.append([\"\", total_surprisal])\n",
    "    return surprisal_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c9ee57-5698-41ab-865c-a3c3903b6f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5, 1: 0.6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict([(0, 0.5), (1, 0.6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78be6634-5eb3-4ddf-aa0a-c4b7f4f5ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Social Issues</th>\n",
       "      <th>Prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>COVID-19 vaccines are dangerous and contain ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vaccine</td>\n",
       "      <td>Dr. Fauci is a lucrative scientist who wants t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump</td>\n",
       "      <td>Most claims made by Donald Trump are false and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump</td>\n",
       "      <td>Donald Trump is a supporter of racism and whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump</td>\n",
       "      <td>Trump has changed the Republican Party for the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Social Issues                                            Prompts\n",
       "0       Vaccine  COVID-19 vaccines are dangerous and contain ha...\n",
       "1       Vaccine  Dr. Fauci is a lucrative scientist who wants t...\n",
       "2         Trump  Most claims made by Donald Trump are false and...\n",
       "3         Trump  Donald Trump is a supporter of racism and whit...\n",
       "4         Trump  Trump has changed the Republican Party for the..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/prompts/extreme_manual_issues.txt\", names=['Social Issues', 'Prompts'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6272da3-fd95-42a8-91e9-aaf496abdc4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 1: surprisal scores\n",
    "    - are surprisal scores of target words significantly different for two models? \n",
    "    - TODO: statistical tests to disentangle the political effect on the target words\n",
    "    - TODO: generate alternative sentences with A-Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7a4072-c1e6-4474-b41a-594f6aefd4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "Returning all unauthorized immigrants to their native countries.\n",
      "            token republican_surprisal  democrat_surprisal\n",
      "0          Return   18.789451715702405   18.27659225579704\n",
      "1             ing    3.286054634746302   5.153398394384503\n",
      "2            Ġall   12.304750677399142  14.039372446452775\n",
      "3   Ġunauthorized    14.23625524622642  15.274388477700791\n",
      "4     Ġimmigrants    4.566481922905242   4.451722160776913\n",
      "5             Ġto   1.4136684792066356  2.3486156215037512\n",
      "6          Ġtheir   1.9953395824046045   5.006285535864182\n",
      "7         Ġnative    7.523945549858044  6.0921690804564745\n",
      "8      Ġcountries   2.3570432433540414   2.083668566341426\n",
      "9               .     9.44965163097566   9.801603071167136\n",
      "10                           75.922643           82.527816\n",
      "\n",
      "====\n",
      "We need to return all unauthorized immigrants to their native countries.\n",
      "            token republican_surprisal  democrat_surprisal\n",
      "0              We    9.083263386776654   7.868435330367907\n",
      "1           Ġneed    4.326925721241488    8.61558221571487\n",
      "2             Ġto   0.9983911398033561  0.9670726456948701\n",
      "3         Ġreturn    9.368631634197909  10.152042783764992\n",
      "4            Ġall     6.69549144811106   7.007089921248638\n",
      "5   Ġunauthorized   14.869204486083925     15.435173912193\n",
      "6     Ġimmigrants   2.1819754176405124  2.4017657200812277\n",
      "7             Ġto   0.9042870114531653   1.563768457152767\n",
      "8          Ġtheir   0.5807237290005633  1.0915272063992352\n",
      "9         Ġnative    5.904312601620996  5.2962755041904686\n",
      "10     Ġcountries   1.7361624581312327  1.6046431623491877\n",
      "11              .    8.667311758720693   8.576464846800054\n",
      "12                           65.316681           70.579842\n",
      "\n",
      "====\n",
      "We need to return all unauthorized immigrants to their native countries.\n",
      "            token republican_surprisal  democrat_surprisal\n",
      "0              We    9.083263386776654   7.868435330367907\n",
      "1           Ġneed    4.326925721241488    8.61558221571487\n",
      "2             Ġto   0.9983911398033561  0.9670726456948701\n",
      "3         Ġreturn    9.368631634197909  10.152042783764992\n",
      "4            Ġall     6.69549144811106   7.007089921248638\n",
      "5   Ġunauthorized   14.869204486083925     15.435173912193\n",
      "6     Ġimmigrants   2.1819754176405124  2.4017657200812277\n",
      "7             Ġto   0.9042870114531653   1.563768457152767\n",
      "8          Ġtheir   0.5807237290005633  1.0915272063992352\n",
      "9         Ġnative    5.904312601620996  5.2962755041904686\n",
      "10     Ġcountries   1.7361624581312327  1.6046431623491877\n",
      "11              .    8.667311758720693   8.576464846800054\n",
      "12                           65.316681           70.579842\n",
      "\n",
      "====\n",
      " Drinking water should be clean\n",
      "       token republican_surprisal  democrat_surprisal\n",
      "0  ĠDrinking   18.353957188407918  18.161449274927524\n",
      "1     Ġwater    7.662690203455353  12.978672120081027\n",
      "2    Ġshould    7.345271877061847   6.143127425825644\n",
      "3        Ġbe    0.989427845885818  0.8201791196180881\n",
      "4     Ġclean    8.849916976928263   6.777956745694821\n",
      "5                       43.201264           44.881385\n",
      "\n",
      "====\n",
      " Milk is white\n",
      "    token republican_surprisal  democrat_surprisal\n",
      "0   ĠMilk   21.124347189085917  20.153638371009908\n",
      "1     Ġis    5.655026633174703   6.831285346747471\n",
      "2  Ġwhite   11.152369405664755  12.331059414433872\n",
      "3                    37.931743           39.315983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/prompts/anes_pilot.txt\", names=['Social Issues', 'Prompts'])\n",
    "\n",
    "\n",
    "def run_single_prompt(prompt):\n",
    "    print(\"====\")\n",
    "    print(prompt)\n",
    "    # print(\"Democratic:\")\n",
    "    dem_sup_list = show_word_surprisals(prompt, dem_model, dems_tokenizer)\n",
    "    rep_sup_list = show_word_surprisals(prompt, repub_model, repub_tokenizer)\n",
    "\n",
    "    df_dem = pd.DataFrame(dem_sup_list, columns=['token', 'democrat_surprisal'])\n",
    "    df_rep = pd.DataFrame(rep_sup_list, columns=['token', 'republican_surprisal'])[['republican_surprisal']]\n",
    "\n",
    "    df_merge = pd.concat([df_dem, df_rep], axis=1)[['token', 'republican_surprisal', 'democrat_surprisal']]\n",
    "    print(df_merge)\n",
    "    print()\n",
    "\n",
    "    \n",
    "for prompt in df.Prompts:\n",
    "    # prompt = \"The Black Lives Matter protesters are extremely peaceful in most cities .\"\n",
    "    run_single_prompt(prompt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e57631-1646-49cf-91c4-a7096f8fb614",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 2: infer political partisanship from tweets\n",
    "    - Given a corpus, we can add surprisal scores of all words to infer its political partisanship\n",
    "    - SUM(surprisals) / N = perplexity\n",
    "    - run two models on the corpus, the one with lower perplexity is the predicted partisan (bayesian inference)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e41e9a61-b51c-4a17-80b1-8a9d46889546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_sentence_perplexity(sentence, model, tokenizer, positions=None):\n",
    "    x = list(score_sentence_token_surprisals(sentence, model, tokenizer))\n",
    "    total_surprisal = 0\n",
    "    surprisal_list = []\n",
    "    for i in range(len(x)):\n",
    "        if positions == None or i in positions:\n",
    "            surprisal_list.append(x[i])\n",
    "            total_surprisal += float(x[i][1])\n",
    "    surprisal_list.append([\"\", total_surprisal])\n",
    "    return total_surprisal / len(x)\n",
    "\n",
    "def predict_partisanship(sentence):\n",
    "    \n",
    "    dem_score = -1 * math.log2(0.6) + compute_sentence_perplexity(sentence, dem_model, dems_tokenizer)\n",
    "    repub_score = -1 * math.log2(0.2) + compute_sentence_perplexity(sentence, repub_model, repub_tokenizer)\n",
    "    \n",
    "    if dem_score < repub_score:\n",
    "        return \"democratic\", dem_score, repub_score\n",
    "    elif dem_score > repub_score:\n",
    "        return \"republican\", dem_score, repub_score\n",
    "    else:\n",
    "        return \"undecided\", dem_score, repub_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59aed31a-8caf-4ef3-9922-4b1f71149e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('democratic', 6.43758195923955, 8.334764597504567)\n",
      "('democratic', 6.994416336287896, 8.152029602709968)\n"
     ]
    }
   ],
   "source": [
    "print(predict_partisanship(\"Abortion is the right of women\"))\n",
    "print(predict_partisanship(\"Abortion is a sin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e915702-4b0d-46e3-a27b-b252710c2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('democratic', 5.421808223325425, 6.783055493974398)\n",
      "('democratic', 5.127494430018432, 6.6606147594837335)\n"
     ]
    }
   ],
   "source": [
    "print(predict_partisanship(\"Trump is the best\"))\n",
    "print(predict_partisanship(\"Biden is the best\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9eeec3-599b-46f6-91e2-34dc1805e776",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 3: Local Phrase Embedding Distance\n",
    "    - measure the local phrase semantic difference between a phrase and its comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6b2b13-7f1f-41c9-9a9e-24fa1580c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_a = \"<|endoftext|> There are many illegal immigrants in the US. <|endoftext|>\"\n",
    "# sentence_b = \"<|endoftext|> There are many undocumented immigrants in the US. <|endoftext|>\"\n",
    "\n",
    "sentence_a = \"<|endoftext|> illegal immigrants <|endoftext|>\"\n",
    "sentence_b = \"<|endoftext|> undocumented immigrants <|endoftext|>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a495db3d-5a63-46a5-af18-431e05a7991f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='../train_lm/models/pretrained_gpt2_2019_dem/', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dems_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a09cf68-7600-4182-ba62-8861742452b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'illegal', 'Ġimmigrants', '<|endoftext|>']\n",
      "['<|endoftext|>', 'und', 'ocumented', 'Ġimmigrants', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(dems_tokenizer.tokenize(sentence_a))\n",
    "print(dems_tokenizer.tokenize(sentence_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73952971-1e16-4c83-ab34-7bc1f0efde80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_phrase_embedding(model, tokenizer, phrase):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(phrase)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    results =  model(input_ids)\n",
    "    return results\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7396355-9647-4267-9989-e538926eff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1562e+01, -1.4179e+01, -7.5818e+00,  ..., -1.0923e+01,\n",
       "          -1.2395e+01, -1.3954e+01],\n",
       "         [-2.8969e+00, -7.6374e+00, -5.4957e-01,  ..., -4.3036e+00,\n",
       "          -5.0997e+00, -1.0699e+01],\n",
       "         [ 4.1435e+00,  7.5750e-03,  2.6001e+00,  ...,  1.2811e+00,\n",
       "           1.8956e+00, -3.1047e+00],\n",
       "         [ 1.4362e+01,  4.9364e+00,  1.8987e+01,  ...,  7.9719e+00,\n",
       "           9.4430e+00,  4.8905e+00]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_phrase_embedding(dem_model, dems_tokenizer, \"sentence_a\").logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2cd1055-515c-491a-ba22-ad393c3c8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_phrases(sentence_a, sentence_b):\n",
    "    sentence_a = \"<|endoftext|> we have \" + sentence_a + \" <|endoftext|>\"\n",
    "    sentence_b = \"<|endoftext|> we have \" + sentence_b + \" <|endoftext|>\"\n",
    "    \n",
    "    dem_a_embedding = extract_phrase_embedding(dem_model, dems_tokenizer, sentence_a).logits.squeeze(0)[-2]  # use the last token\n",
    "    dem_b_embedding = extract_phrase_embedding(dem_model, dems_tokenizer, sentence_b).logits.squeeze(0)[-2]\n",
    "    print(\"Similarity between the two phrases\")\n",
    "    print(\"A:\", sentence_a)\n",
    "    print(\"B:\", sentence_b)\n",
    "    print(\"Dem:\", abs(cos(dem_a_embedding, dem_b_embedding).item()))\n",
    "\n",
    "    rep_a_embedding = extract_phrase_embedding(repub_model, repub_tokenizer, sentence_a).logits.squeeze(0)[-2]  # use the last token\n",
    "    rep_b_embedding = extract_phrase_embedding(repub_model, repub_tokenizer, sentence_b).logits.squeeze(0)[-2]\n",
    "\n",
    "    print(\"Repub:\", abs(cos(rep_a_embedding, rep_b_embedding).item()))\n",
    "    \n",
    "    pretrained_a_embedding = extract_phrase_embedding(gpt2_model, gpt2_tokenizer, sentence_a).logits.squeeze(0)[-2]  # use the last token\n",
    "    pretrained_b_embedding = extract_phrase_embedding(gpt2_model, gpt2_tokenizer, sentence_b).logits.squeeze(0)[-2]\n",
    "    \n",
    "    print(\"Pretrained:\", abs(cos(pretrained_a_embedding, pretrained_b_embedding).item()))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "faa6b4d9-4c20-41f8-b19e-2a5c18f4acb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have undocumented workers <|endoftext|>\n",
      "B: <|endoftext|> we have illegal aliens <|endoftext|>\n",
      "Dem: 0.984472930431366\n",
      "Repub: 0.9796950817108154\n",
      "Pretrained: 0.9999767541885376\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have estate tax <|endoftext|>\n",
      "B: <|endoftext|> we have death tax <|endoftext|>\n",
      "Dem: 0.988421618938446\n",
      "Repub: 0.9874023795127869\n",
      "Pretrained: 0.9999802708625793\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have capitalism <|endoftext|>\n",
      "B: <|endoftext|> we have free market <|endoftext|>\n",
      "Dem: 0.9737823009490967\n",
      "Repub: 0.9740625619888306\n",
      "Pretrained: 0.999916136264801\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have foreign trade <|endoftext|>\n",
      "B: <|endoftext|> we have international trade <|endoftext|>\n",
      "Dem: 0.9951511025428772\n",
      "Repub: 0.9947640895843506\n",
      "Pretrained: 0.9999864101409912\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have public option <|endoftext|>\n",
      "B: <|endoftext|> we have government-run <|endoftext|>\n",
      "Dem: 0.9211022257804871\n",
      "Repub: 0.9528332352638245\n",
      "Pretrained: 0.9998873472213745\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have trickle-down <|endoftext|>\n",
      "B: <|endoftext|> we have cut taxes <|endoftext|>\n",
      "Dem: 0.961600124835968\n",
      "Repub: 0.973410964012146\n",
      "Pretrained: 0.9998621344566345\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have voodoo economics <|endoftext|>\n",
      "B: <|endoftext|> we have supply-side <|endoftext|>\n",
      "Dem: 0.8372368216514587\n",
      "Repub: 0.9763371348381042\n",
      "Pretrained: 0.9998254179954529\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have tax expenditures <|endoftext|>\n",
      "B: <|endoftext|> we have spending programs <|endoftext|>\n",
      "Dem: 0.9719120860099792\n",
      "Repub: 0.9616166353225708\n",
      "Pretrained: 0.9999771118164062\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have waterboarding <|endoftext|>\n",
      "B: <|endoftext|> we have interrogation <|endoftext|>\n",
      "Dem: 0.9606936573982239\n",
      "Repub: 0.9295581579208374\n",
      "Pretrained: 0.9999564290046692\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have socialized medicine <|endoftext|>\n",
      "B: <|endoftext|> we have single-payer <|endoftext|>\n",
      "Dem: 0.9810807704925537\n",
      "Repub: 0.9908479452133179\n",
      "Pretrained: 0.9998708367347717\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have political speech <|endoftext|>\n",
      "B: <|endoftext|> we have campaign spending <|endoftext|>\n",
      "Dem: 0.8588502407073975\n",
      "Repub: 0.9591264724731445\n",
      "Pretrained: 0.9999431371688843\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have star wars <|endoftext|>\n",
      "B: <|endoftext|> we have strategic defense initiative <|endoftext|>\n",
      "Dem: 0.969371497631073\n",
      "Repub: 0.9551512002944946\n",
      "Pretrained: 0.9999110102653503\n",
      "\n",
      "\n",
      "Similarity between the two phrases\n",
      "A: <|endoftext|> we have nuclear option <|endoftext|>\n",
      "B: <|endoftext|> we have constitutional option <|endoftext|>\n",
      "Dem: 0.9180405139923096\n",
      "Repub: 0.8479630947113037\n",
      "Pretrained: 0.9999608993530273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phrase_pairs = [\n",
    "    [\"undocumented workers\", \"illegal aliens\"],\n",
    "    [\"estate tax\", \"death tax\"],\n",
    "    [\"capitalism\",\"free market\"],\n",
    "    [\"foreign trade\", \"international trade\"],\n",
    "    [\"public option\", \"government-run\"],\n",
    "    [\"trickle-down\", \"cut taxes\"],\n",
    "    [\"voodoo economics\", \"supply-side\"],\n",
    "    [\"tax expenditures\", \"spending programs\"],\n",
    "    [\"waterboarding\", \"interrogation\"],\n",
    "    [\"socialized medicine\", \"single-payer\"],\n",
    "    [\"political speech\", \"campaign spending\"],\n",
    "    [\"star wars\", \"strategic defense initiative\"],\n",
    "    [\"nuclear option\", \"constitutional option\"]\n",
    "]\n",
    "\n",
    "for pair in phrase_pairs:\n",
    "    print()\n",
    "    compare_two_phrases(pair[0], pair[1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2410b738-73c4-402d-b1da-76c0fb37a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_two_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08036304-894f-44e2-94ed-9d7c217b1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_two_phrases(\"Monday\", \"Tuesday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d63b39-aef1-4377-8a72-d1991dcd06e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> black people <|endoftext|>\n",
      "B: <|endoftext|> minority group <|endoftext|>\n",
      "Dem: 0.9661055207252502\n",
      "Repub: 0.926490068435669\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"black people\", \"minority group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4181a8ce-060f-4fb4-b320-0308407d9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_two_phrases(\"Black American\", \"African American\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a3cfc1c3-e7d0-4fea-aeb7-cc105c2377c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_two_phrases(\"Hispanic American\", \"Latino American\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b969f415-6586-45a8-9fc8-3d0f8c48af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> communism <|endoftext|>\n",
      "B: <|endoftext|> socialism <|endoftext|>\n",
      "Dem: 0.7550395727157593\n",
      "Repub: 0.8931132555007935\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"communism\", \"socialism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0a9d9b8-15d6-4775-b2af-6a22b1f819da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> illegal aliens <|endoftext|>\n",
      "B: <|endoftext|> undocumented workers <|endoftext|>\n",
      "Dem: 0.9582004547119141\n",
      "Repub: 0.9856297373771667\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"illegal aliens\", \"undocumented workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9ad570-c580-4f4a-a61e-2497fea2bec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> estate tax <|endoftext|>\n",
      "B: <|endoftext|> death tax <|endoftext|>\n",
      "Dem: 0.9430157542228699\n",
      "Repub: 0.9681082963943481\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"estate tax\", \"death tax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6224aad-b340-46b2-b7c0-575580c9742c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> black people <|endoftext|>\n",
      "B: <|endoftext|> minority group <|endoftext|>\n",
      "Dem: 0.9661055207252502\n",
      "Repub: 0.926490068435669\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"black people\", \"minority group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23a0476-3346-4854-bddd-ba57ec7e3fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> affordable care act <|endoftext|>\n",
      "B: <|endoftext|> obama care <|endoftext|>\n",
      "Dem: 0.9673094749450684\n",
      "Repub: 0.9548484086990356\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"affordable care act\", \"obama care\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a979dc97-6379-4f1d-93f2-aa89a9fe63e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> i need water <|endoftext|>\n",
      "B: <|endoftext|> i need bread <|endoftext|>\n",
      "Dem: 0.9507384300231934\n",
      "Repub: 0.9368794560432434\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"i need water\", \"i need bread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7444e98-8ad9-473e-85e6-6cef9a2b081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> all lives matter <|endoftext|>\n",
      "B: <|endoftext|> black lives do matter <|endoftext|>\n",
      "Dem: 0.8970802426338196\n",
      "Repub: 0.9866625666618347\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"all lives matter\", \"black lives do matter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "857251a1-f15f-4023-9c3d-3810b8eecbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> all lives matter <|endoftext|>\n",
      "B: <|endoftext|> black and white lives both matter <|endoftext|>\n",
      "Dem: 0.17848418653011322\n",
      "Repub: 0.48774904012680054\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"all lives matter\", \"black and white lives both matter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3acfe21-dc24-493e-8e90-792b9f9727d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> muslims <|endoftext|>\n",
      "B: <|endoftext|> terrorists <|endoftext|>\n",
      "Dem: 0.9718367457389832\n",
      "Repub: 0.9869581460952759\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"muslims\", \"terrorists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a13d6f8d-552d-49f9-98f4-fbb4ab471a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> gun control <|endoftext|>\n",
      "B: <|endoftext|> gun safety <|endoftext|>\n",
      "Dem: 0.9832996129989624\n",
      "Repub: 0.9932688474655151\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"gun control\", \"gun safety\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "625473fd-b8fc-4f1a-8551-8cf6a02dfec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> liberal people <|endoftext|>\n",
      "B: <|endoftext|> radical people <|endoftext|>\n",
      "Dem: 0.710970938205719\n",
      "Repub: 0.9705708622932434\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"liberal people\", \"radical people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53a4329e-dc1a-43ff-8fb0-e992183be231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> liberal people <|endoftext|>\n",
      "B: <|endoftext|> leftist people <|endoftext|>\n",
      "Dem: 0.9099975228309631\n",
      "Repub: 0.9827098846435547\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"liberal people\", \"leftist people\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47e85733-8c40-4a5e-9604-7133c91afebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> liberal people <|endoftext|>\n",
      "B: <|endoftext|> socialist people <|endoftext|>\n",
      "Dem: 0.9500482082366943\n",
      "Repub: 0.9805581569671631\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"liberal people\", \"socialist people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e90e3d8a-afcf-44f5-8749-775d0143d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> homosexual people <|endoftext|>\n",
      "B: <|endoftext|> gay people <|endoftext|>\n",
      "Dem: 0.9419152140617371\n",
      "Repub: 0.911022424697876\n"
     ]
    }
   ],
   "source": [
    "# https://www.albertahealthservices.ca/assets/info/pf/div/if-pf-div-terms-and-phrases-to-avoid.pdf\n",
    "compare_two_phrases(\"homosexual people\", \"gay people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "35df4046-71b6-49c5-885d-7d196054a790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the two phrases\n",
      "A: <|endoftext|> affordable care act <|endoftext|>\n",
      "B: <|endoftext|> Obama care <|endoftext|>\n",
      "Dem: 0.9751651287078857\n",
      "Repub: 0.944922149181366\n"
     ]
    }
   ],
   "source": [
    "compare_two_phrases(\"affordable care act\", \"Obama care\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe2b9d-8587-4ce2-9455-9af809543529",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiment 4: opinion generation\n",
    "    - generate opinions with each GPT-2, ~ 20 opinions\n",
    "    - use sentiment analysis tool to quantify the attitude of the group towards:\n",
    "        - social issues\n",
    "        - in-group, out-group attitudes\n",
    "        - political/public figures\n",
    "    - check against gold stance of the group\n",
    "\n",
    "Related works: \n",
    "1. [Mining Insights from Large Scale Corpora Using Fine Tuned Language Models](https://ebooks.iospress.nl/volumearticle/55101)\n",
    "2. [Analyzing COVID-19 Tweets with Transformer-based Language Models](https://arxiv.org/abs/2104.10259)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7136c91-e164-4aaa-b1d8-a11aa4883154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "dem_generator = pipeline('text-generation', model='../models/pretrained_gpt2_dems_4M_full_data_single_follow/')\n",
    "repub_generator = pipeline('text-generation', model='../models/gpt2_repubs_full_data_single_follow/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d93a9617-359b-4977-9a1b-6be7dc79f4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='../models/pretrained_gpt2_dems_4M_full_data_single_follow/checkpoint-489680/', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dems_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4afaaab9-4702-463b-9c8e-fee02809d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "def generate_with_a_prompt(prompt, dem_gen, repub_gen):\n",
    "    # dem gen\n",
    "    print(\"Democratic GPT-2:\")\n",
    "    results = dem_gen(prompt, max_length=20, temperature=0.4, num_return_sequences=5, pad_token_id=50256, clean_up_tokenization_spaces=True)\n",
    "    for res in results:\n",
    "        print(res['generated_text'].replace(\"\\n\", \" \"))\n",
    "    \n",
    "    # repub gen\n",
    "    print(\"========\\n\\n\")\n",
    "    print(\"Republican GPT-2:\")\n",
    "    results = repub_gen(prompt, max_length=20, temperature=0.4, num_return_sequences=5, pad_token_id=50256, clean_up_tokenization_spaces=True)\n",
    "    for res in results:\n",
    "        print(res['generated_text'].replace(\"\\n\", \" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "d5c981f8-9f50-4c2b-a051-65319edfa5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dems_tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4fd9ec75-bb36-41a9-ae93-bef42a4668c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4467cad2-bc27-4f74-8943-c4b506216f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr [('I', 0.08102313429117203), ('@', 0.07565118372440338), (',', 0.04771842807531357), ('to', 0.036704517900943756), ('.', 0.02933499589562416), ('@', 0.021787913516163826), ('Dr', 0.000271661119768396)]\n",
      ". [('inking', 0.13614609837532043), ('ink', 0.13268722593784332), ('unk', 0.06880466639995575), ('ump', 0.06106007471680641), ('inks', 0.06021980941295624), ('one', 0.0600702166557312), ('.', 3.738446321222e-05)]\n",
      "ĠAnthony [('Jack', 0.02079404518008232), ('Kelly', 0.019364500418305397), ('M', 0.017932597547769547), ('G', 0.01788846217095852), ('Joe', 0.01640082150697708), ('Mart', 0.0161176435649395), ('ĠAnthony', 4.877291303273523e-06)]\n",
      "ĠF [('F', 0.7397092580795288), ('Evans', 0.009879098273813725), ('Hopkins', 0.008779113180935383), ('Stro', 0.007220400031656027), ('S', 0.006825625896453857), ('M', 0.005388324148952961), ('ĠF', 0.7397092580795288)]\n",
      "au [('au', 0.9944012761116028), ('.', 0.0009878984419628978), ('oy', 0.0002599698200356215), ('inc', 0.0002558085834607482), ('enn', 0.00017853720055427402), ('ara', 0.00015103605983313173)]\n",
      "ci [('ci', 0.9988486766815186), ('qu', 0.0002668238594196737), ('cci', 0.00020561466226354241), ('co', 7.273288065334782e-05), ('chi', 6.215314351720735e-05), ('cher', 5.828566281707026e-05)]\n",
      "Ġis [('to', 0.08061342686414719), (':', 0.0626722127199173), ('Ċ', 0.05940626934170723), (',', 0.05255284905433655), ('is', 0.036994628608226776), ('says', 0.035556286573410034), ('Ġis', 0.036994628608226776)]\n",
      "Ġa [('the', 0.07752518355846405), ('a', 0.07511161267757416), ('one', 0.03284814953804016), ('on', 0.03282444179058075), ('not', 0.026578621938824654), ('speaking', 0.025770075619220734), ('Ġa', 0.07511161267757416)]\n",
      "Ġhero [('hero', 0.1035597175359726), ('true', 0.03634202852845192), ('national', 0.020792411640286446), ('physician', 0.02055230922996998), ('great', 0.01929059997200966), ('doctor', 0.016611972823739052), ('Ġhero', 0.1035597175359726)]\n",
      "Ġ. [('.', 0.31219521164894104), ('of', 0.09638430178165436), ('for', 0.09061645716428757), ('and', 0.08940090984106064), ('!', 0.0672082006931305), (',', 0.06691739708185196), ('Ġ.', 0.31219521164894104)]\n"
     ]
    }
   ],
   "source": [
    "def extract_topk_per_token(prompt, model, tokenizer):\n",
    "    \n",
    "    input_ids = torch.tensor(tokenizer.encode(tokenizer.bos_token) + \n",
    "                             tokenizer.encode(prompt)).unsqueeze(0)\n",
    "    input_length = len(input_ids[0])\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "    all_list = []\n",
    "    target_tokens = []\n",
    "    for i in range(input_length-1):\n",
    "        \n",
    "        token_probs_list = []\n",
    "        \n",
    "        target_token_id = input_ids[0][i+1]\n",
    "        target_token = tokenizer.convert_ids_to_tokens(target_token_id.item())\n",
    "        # print(target_token)\n",
    "        \n",
    "        each_prob_dist = torch.nn.Softmax(0)(outputs[0].squeeze()[i])\n",
    "        # print(each_prob_dist.shape, each_prob_dist.sum())\n",
    "        topk = torch.topk(each_prob_dist, 6)\n",
    "\n",
    "        topk_probas = topk.values.detach().tolist()\n",
    "        topk_words = tokenizer.convert_ids_to_tokens(topk.indices)\n",
    "        topk_words = [each.replace(\"Ġ\", \"\") for each in topk_words]\n",
    "        token_probs_list.extend(list(zip(topk_words, topk_probas)))\n",
    "        if target_token not in set(topk_words):\n",
    "            token_probs_list.append((target_token, each_prob_dist[target_token_id].item()))\n",
    "#         # merge common words\n",
    "#         word2proba = {}\n",
    "#         for word, proba in token_probs_list:\n",
    "#             if word in word2proba:\n",
    "#                 word2proba[word] += proba\n",
    "#             else:\n",
    "#                 word2proba[word] = proba\n",
    "#         token_probs_list = sorted(list(word2proba.items()), key=lambda x: -x[1])\n",
    "        all_list.append(token_probs_list)\n",
    "        target_tokens.append(target_token)\n",
    "    return target_tokens, all_list\n",
    "\n",
    "target_tokens, all_list = extract_topk_per_token(\"Dr. Anthony Fauci is a hero .\", dem_model, dems_tokenizer)\n",
    "\n",
    "for target_token, each in zip(target_tokens, all_list):\n",
    "    print(target_token, each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a489f334-a435-43f6-96ce-3abf5b84cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most 13.51%\n",
      "biggest 7.10%\n",
      "largest 7.07%\n",
      "only 5.90%\n",
      "ones 5.64%\n",
      "backbone 3.99%\n",
      "Ġmost 13.51%\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"Labor unions are the most\"\n",
    "\n",
    "target_tokens, all_list = extract_topk_per_token(example_sentence, gpt2_model, gpt2_tokenizer)\n",
    "\n",
    "alternative_tops = all_list[-1]\n",
    "target_token = target_token[-1]\n",
    "for word, prob in alternative_tops:\n",
    "    print(word, \"{0:.2%}\".format(prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd39e137-e382-4d81-823a-dd562bb334c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most 10.67%\n",
      "largest 7.72%\n",
      "worst 5.02%\n",
      "backbone 4.90%\n",
      "ones 4.68%\n",
      "biggest 4.41%\n",
      "Ġmost 10.67%\n"
     ]
    }
   ],
   "source": [
    "target_tokens, all_list = extract_topk_per_token(example_sentence, repub_model, repub_tokenizer)\n",
    "\n",
    "alternative_tops = all_list[-1]\n",
    "target_token = target_token[-1]\n",
    "for word, prob in alternative_tops:\n",
    "    print(word, \"{0:.2%}\".format(prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d833162-07bd-4311-a9a2-b19edb4c3b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone 22.37%\n",
      "only 10.91%\n",
      "most 6.98%\n",
      "largest 4.49%\n",
      "best 3.60%\n",
      "ones 1.98%\n",
      "Ġmost 6.98%\n"
     ]
    }
   ],
   "source": [
    "target_tokens, all_list = extract_topk_per_token(example_sentence, dem_model, dems_tokenizer)\n",
    "\n",
    "alternative_tops = all_list[-1]\n",
    "target_token = target_token[-1]\n",
    "for word, prob in alternative_tops:\n",
    "    print(word, \"{0:.2%}\".format(prob))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2d578911-2d2e-45e1-a4f3-c887aec73827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem_generator = pipeline('text-generation', model='gpt2', device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "339137e5-9aac-44ed-88c2-117f48621eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "Donald Trump is a racist, misogynistic, xenophobic, homophobic, and misogynist. He '\n",
      "Donald Trump is a loser. A loser who lies and cheats. A loser who lies and che\n",
      "Donald Trump is a liar, a cheat, a fraud, a liar, a cheat, a liar\n",
      "Donald Trump is a horrible person. He's a liar, a cheat, a crook,\n",
      "Donald Trump is a liar. He's lying. He's lying. He's lying\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "Donald Trump is a winner. He is fighting for us. @USER @USER @USER @\n",
      "Donald Trump is a racist and a fascist. \"@USER @USER @USER @USER @\n",
      "Donald Trump is a racist. \"@USER @USER @USER @USER @USER @USER\n",
      "Donald Trump is a great American and a great President! @USER @USER @USER @USER\n",
      "Donald Trump is a liar. He is a liar. \"@USER @USER @USER @\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"Donald Trump is a\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ccd536b6-8d1f-40e2-9836-b6959c73fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "Dr. Fauci is a hero. He saved lives. He deserves a medal. \"\n",
      "Dr. Fauci is a hero. He's a scientist. He's a leader\n",
      "Dr. Fauci is a hero. He is a leader in the fight against the virus.\n",
      "Dr. Fauci is a hero. He's a true public servant. He's\n",
      "Dr. Fauci is a hero. He's been there before. He's done\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "Dr. Fauci is a liar, a fraud and a fraud. HTTPURL\" @\n",
      "Dr. Fauci is a liar, a fraud, and a fraud. He's a\n",
      "Dr. Fauci is a fraud and a liar. \"@USER @USER @USER\n",
      "Dr. Fauci is a fraud and a liar. @USER @USER @USER @\n",
      "Dr. Fauci is a liar and a fraud. @USER @USER @USER @\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"Dr. Fauci is a\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "28ba0652-1129-4902-bb0c-fbb6e51cd962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "The pandemic was caused by the government's incompetence, not by the people who work for it\n",
      "The pandemic was caused by a lack of leadership, not by a lack of money. The pand\n",
      "The pandemic was caused by the government not being able to do anything about the virus. \"\n",
      "The pandemic was caused by the government's failure to protect its citizens from the worst of the\n",
      "The pandemic was caused by the Trump administration's refusal to take action against the Chinese government.\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "The pandemic was caused by the government's overreach, and it's not going away\n",
      "The pandemic was caused by the media's fear of the virus. HTTPURL @USER\n",
      "The pandemic was caused by the government's failure to secure the borders. HTTPURL @\n",
      "The pandemic was caused by the government's unwillingness to protect the people. @USER @\n",
      "The pandemic was caused by the government's failure to act quickly and decisively. HTTPURL \n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"The pandemic was caused by\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3d7268f5-405b-4ac1-9cd6-d45a6c1e1d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "For the pandemic, we blame the #TrumpVirus on the #GOP and #Republicans who\n",
      "For the pandemic, we blame @USER for not being able to provide #COVID19 testing\n",
      "For the pandemic, we blame the state and local governments for the lack of funding to provide the\n",
      "For the pandemic, we blame our own failure to control the virus and the failure of our leaders\n",
      "For the pandemic, we blame the people who are in charge. We blame the people who are\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "For the pandemic, we blame the government, not the people. HTTPURL\" \"@\n",
      "For the pandemic, we blame the government for the problems we face. We blame the government for\n",
      "For the pandemic, we blame the media, the teachers union, and the politicians. But we\n",
      "For the pandemic, we blame the media for the problems in our country. They are the ones\n",
      "For the pandemic, we blame the coronavirus on the people who haven't been vaccinated\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"For the pandemic, we blame\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f34c6995-d53b-4db4-9e95-5191bc9a2dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "During the pandemic, Asians have been feeling forced to work less. Learn more about the critical need\n",
      "During the pandemic, Asians have been feeling less xenophobic than many other Americans. But the virus\n",
      "During the pandemic, Asians have been feeling extra targeted. HTTPURL\" @USER I '\n",
      "During the pandemic, Asians have been feeling the brunt of anti-Asian violence, and it has\n",
      "During the pandemic, Asians have been feeling ‘ underrepresented'in education HTTPURL\" \n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "During the pandemic, Asians have been feeling the pinch more frequently than the other way around, according\n",
      "During the pandemic, Asians have been feeling helpless and depressed. But now there's an opportunity\n",
      "During the pandemic, Asians have been feeling left out on the list. Learn how your organization can\n",
      "During the pandemic, Asians have been feeling isolation from the rest of the U. S. population\n",
      "During the pandemic, Asians have been feeling particularly alienated as leaders have sought to curb tensions. The\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"During the pandemic, Asians have been feeling\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cf46370-6917-4b09-a608-d24a6707c001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "During the pandemic, Black folks have been feeling their own pain, and that impacts the entire US\n",
      "During the pandemic, Black folks have been feeling invisible and unseen. Our health is most important to\n",
      "During the pandemic, Black folks have been feeling overlooked due to lack of resources, so @USER\n",
      "During the pandemic, Black folks have been feeling overlooked and underrepresented because they have no job.\n",
      "During the pandemic, Black folks have been feeling invisible, as have their communities, and the effects\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "During the pandemic, Black folks have been feeling trapped in urban communities. They are now struggling to\n",
      "During the pandemic, Black folks have been feeling the impact of an online shopping spree and the need\n",
      "During the pandemic, Black folks have been feeling threatened at restaurants, shops, gyms & grocery\n",
      "During the pandemic, Black folks have been feeling the brunt of social justice activism and victimization :\n",
      "During the pandemic, Black folks have been feeling hopeless & isolated. Instead of a financial well-\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"During the pandemic, Black folks have been feeling\", dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "784e0b82-0dda-4822-8aca-dd13bab284cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "The police brutality is what people talk about and is why the police are on the wrong side when it\n",
      "The police brutality shouldn't exist. We can't reform this. Please do something or just vote\n",
      "The police brutality that occurred yesterday wasn't the same as the police brutality that occured on the\n",
      "The police brutality that took place in front of him yesterday is truly disgusting and disgusting. #BLM\n",
      "The police brutality / murder / war on peaceful protestors is never going to stop. #BlackLives\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "The police brutality narrative was always meant to get rid of the police. Now it's a big\n",
      "The police brutality narrative is a farce. The only way to stop it is to prosecute the criminals\n",
      "The police brutality of the BLM / ANTIFA Riots, is not a “ systemic threat\n",
      "The police brutality narrative is not about police brutality, because that's the narrative\" @USER\n",
      "The police brutality? What is BLM doing to protect their lives, you fucking clown\" \"@\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"The police brutality\", \n",
    "                       dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88ae791c-2690-4a17-8f29-a881c78de464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "Abortion is not a legal right. Not even close to as well as it should be. #\n",
      "Abortion is the law of the land. Why would someone with the power to decide what happens to\n",
      "Abortion is only “ if it is illegal. ” If pregnancy and abortion aren't\n",
      "Abortion is #Life and #Choice : HTTPURL HTTPURL You might just want to start\n",
      "Abortion isn't a constitutional right but like, you can have a vagina in your choice without\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "Abortion is not okay. I'm proud of our military. \"It's a\n",
      "Abortion is the most horrific thing he does but I'm certain he never has to apologize for\n",
      "Abortion is barbaric, and the #AbortionHealing is worse. #EndAbortionNow\n",
      "Abortion is a disgusting act, even though it's actually pro-life. It's\n",
      "Abortion is no excuse. The majority of abortions is a horrific tragedy. So I think that '\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"Abortion is\", \n",
    "                       dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7771b934-7fd0-4478-abc0-7d84652227f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "Joe Biden is at it again. He's making sure that a majority can vote for something instead\n",
      "Joe Biden is the only one who hasn't publicly denounced any of the terrorists, including #Tra\n",
      "Joe Biden is your president. But you gotta get him the fuck outta here ASAP and I hope\n",
      "Joe Biden is the first American president to win the popular vote twice in history. ” A must\n",
      "Joe Biden is the President-elect and is in control of the Senate. Do something. \"\n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "Joe Biden is a horrible person. The man is a criminal and has ties to the Ukraine, China\n",
      "Joe Biden is in a world of his own. He wants to lock in to what happened at the\n",
      "Joe Biden is the definition of the radical left ”... @USER What's good to\n",
      "Joe Biden is too old to remember his name. @USER @USER @USER @USER @\n",
      "Joe Biden is the president-elect at that point. \"@USER It wouldn't matter\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"Joe Biden is\", \n",
    "                       dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d0dd37b-4f39-4572-a7ce-e3c29b2a3c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democratic GPT-2:\n",
      "The Black Lives Matter protesters aren't just anti-racist, they're a unionized movement\n",
      "The Black Lives Matter protesters are protesting police brutality and using that force to control them. #blm\n",
      "The Black Lives Matter protesters are at the intersection of the 5 and I - 5 in #Minneapolis\n",
      "The Black Lives Matter protesters are standing in solidarity with the local sheriff's offices. #BLM\n",
      "The Black Lives Matter protesters are about not being murdered or being beaten. #BLM HTTPURL \n",
      "========\n",
      "\n",
      "\n",
      "Republican GPT-2:\n",
      "The Black Lives Matter protesters are a domestic terrorist organization. The left's anti-police rhetoric is\n",
      "The Black Lives Matter protesters are the only ones who have a clue about what's going on.\n",
      "The Black Lives Matter protesters are violent, looting, and breaking through barriers of a federal courthouse in Oregon\n",
      "The Black Lives Matter protesters are not being harassed by BLM rioters? :thinking_face: \n",
      "The Black Lives Matter protesters are'not being represented'in the US Supreme Court HTTPURL. @\n"
     ]
    }
   ],
   "source": [
    "generate_with_a_prompt(\"The Black Lives Matter protesters are\", \n",
    "                       dem_generator, repub_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda2d3cb-1f24-4e04-bc05-7506229a9500",
   "metadata": {},
   "source": [
    "## Experiment 5: Writing Assitive Tool \n",
    "\n",
    "[Huggingface GPT-2 Demo](https://transformer.huggingface.co/doc/gpt2-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "56f5fb6b-2188-454b-8d49-c2eee43bd157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to talk to a science denier'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book = \"How to Talk to a Science Denier\".lower()\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "920a9534-3bc6-46d0-8dac-e2b87923cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "how to talk to a science denier\n",
      "      token  republican_surprisal    democrat_surprisal\n",
      "0       how    15.580385013795645     17.03848353716829\n",
      "1       Ġto     5.289046675146581     7.217765604016828\n",
      "2     Ġtalk     9.770761787872772     8.448259825352013\n",
      "3       Ġto    1.7981924019495104    2.2361055446201403\n",
      "4        Ġa    2.3296321467084917     4.377495907388955\n",
      "5  Ġscience    12.281267302298826     11.70924018936432\n",
      "6      Ġden     4.253549543299569      5.64415561039224\n",
      "7       ier  0.014942088113823913  0.020963625417505335\n",
      "8                       51.317777              56.69247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_single_prompt(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ffa70-97e4-43a8-b48c-627d0097520a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
